<div class="backprop-demo">
  <h2>Interactive Backpropagation</h2>
  <p class="bp-intro">
    This visual demonstration shows how backpropagation works step by step in a neural network.
    Click "Next Step" to advance through each phase of the algorithm.
  </p>

  <div class="bp-toolbar">
    <button id="bp-reset">ğŸ”„ Reset</button>
    <button id="bp-step">â–¶ Next Step</button>
    <button id="bp-prev" disabled>â—€ Prev Step</button>
    <button id="bp-play">â¯ Auto Play</button>
    
    <span class="bp-inline">
      Learning Rate: <code id="lr-value">0.5</code>
      <input type="range" id="lr" min="0.1" max="1" step="0.1" value="0.5">
    </span>
    
    <span class="bp-inline">
      Speed: <code id="speed-value">1000ms</code>
      <input type="range" id="speed" min="500" max="3000" step="250" value="1000">
    </span>
    
    <span class="bp-inline">
      Error: <code id="bp-loss">-</code>
    </span>
  </div>

  <div class="bp-phase-label" id="bp-phase-label">Step 1: Initialize Network</div>

  <div class="bp-canvas-wrapper">
    <div class="bp-equation-overlay" id="bp-equation-overlay"></div>
    <canvas id="bp-network" width="700" height="400"></canvas>
  </div>

  <!-- Interactive Questions Section -->
  <div class="bp-questions">
    <h3>ğŸ¤” Explore and Learn</h3>
    <details>
      <summary>ğŸ’¡ QuÃ¨ Ã©s la funciÃ³ sigmoidea?</summary>
      <p>La funciÃ³ sigmoidea Ïƒ(x) = 1/(1+e^(-x)) Ã©s una funciÃ³ d'activaciÃ³ que mapeja qualsevol nÃºmero real a un valor entre 0 i 1. Ã‰s especialment Ãºtil perquÃ¨ tÃ© una derivada simple: Ïƒ'(x) = Ïƒ(x)(1-Ïƒ(x)).</p>
    </details>
    
    <details>
      <summary>ğŸ“ˆ Com funciona la taxa d'aprenentatge?</summary>
      <p>La taxa d'aprenentatge (Î±) controla la magnitud dels canvis en els pesos durant l'entrenament. Una taxa alta pot fer que l'algoritme convergeixi rÃ pidament perÃ² pot saltar el mÃ­nim Ã²ptim. Una taxa baixa Ã©s mÃ©s estable perÃ² mÃ©s lenta.</p>
    </details>
    
    <details>
      <summary>ğŸ¯ Per quÃ¨ utilitzem XOR?</summary>
      <p>XOR Ã©s un problema clÃ ssic que no es pot resoldre amb una sola neurona (no Ã©s linealment separable). Demostra la necessitat de capes ocultes en les xarxes neuronals per aprendre funcions no lineals.</p>
    </details>
  </div>
</div>

<script>
document.addEventListener('DOMContentLoaded', () => {
  new BackpropDemo();
});
</script>
