---
layout: post
title: "El Primer LLM"
date: 2025-03-23
categories: genai llm machine-learning
summary: Jeremy Howard i Sebastian Ruder van crear el primer LLM.
permalink: /blog/the-first-llm
---

<figure style="margin: 0; margin-bottom: 1em;">
  <img
    src="/images/gpt-1-illustration.webp"
    alt="Il¬∑lustraci√≥: Ben Barry (il¬∑lustraci√≥ de portada de l'anunci de GPT-1)"
    style="border-radius: 0.4em;"
    width="100%"
    height="auto"
    style="aspect-ratio: 16/9; object-fit: cover; border-radius: 0.4em;"
  >
  <figcaption style="color: #777;">Il¬∑lustraci√≥: Ben Barry</figcaption>
</figure>

<div id="toc" style="background: #f8f9fa; padding: 1em; border-radius: 0.4em; position: absolute; right: calc(50% - 45em); top: 54em; width: 15em; max-height: 80vh; overflow-y: auto;">
  <div id="toc-content"></div>
</div>

<script>document.addEventListener('DOMContentLoaded', function() {
  if (window.innerWidth < 768) {
    document.getElementById('toc').style.display = 'none';
    return;
  }

  const toc = document.getElementById('toc');
  const headings = document.querySelectorAll('h2, h3');
  const tocContent = document.getElementById('toc-content');
  const lastTwoHeadingIndexes = new Set([headings.length - 1, headings.length - 2]);

  headings.forEach((heading, index) => {
    if (lastTwoHeadingIndexes.has(index)) {
      return;
    }
    if (!heading.id) {
      heading.id = `heading-${index}`;
    }
    const link = document.createElement('a');
    link.href = `#${heading.id}`;
    link.textContent = heading.textContent;
    link.style.color = '#777';
    const div = document.createElement('div');
    div.appendChild(link);
    if (heading.tagName === 'h3') {
      div.style.marginLeft = '1.5em';
    }
    div.style.marginBottom = '0.5em';
    tocContent.appendChild(div);
  });

  /* Update TOC visibility on window resize */
  window.addEventListener('resize', function() {
    document.getElementById('toc').style.display = window.innerWidth < 768 ? 'none' : 'block';
  });
});</script>


Qu√® estaves fent mentre la revoluci√≥ dels LLM naixia i en pocs anys va refer la ind√∫stria de la computaci√≥?

Va ser el primer LLM realment fet per un australi√†, entre sessions de surf?

Aquestes preguntes m'han acompanyat des que vaig revisar, cronol√≤gicament, els articles de recerca que van escalar el modelatge del llenguatge i la capitalitzaci√≥ de mercat de la nostra ind√∫stria gegant en bilions de d√≤lars m√©s.

'Nosaltres' ho vam fer. El benchmark m√©s fam√≥s de la hist√≤ria de la computaci√≥, el de Turing, ara √©s *massa f√†cil*. Avui, desenes de nous benchmarks mesuren la frontera en expansi√≥ dels √®xits de la IA: pot guanyar el Putnam, pot v√®ncer Pok√©mon?

El 2016, un any abans que nasqu√©s el 'primer LLM', estava assegut a classes de ci√®ncies de la computaci√≥ sentint professors parlar de la jerarquia de les gram√†tiques de Chomsky. En aquestes confer√®ncies, utilitzaven la prova de Turing per injectar una mica de meravella, una mica de misticisme en la mec√†nica seca dels aut√≤mats de pila. Alguna vegada far√≠em un programa que super√©s aquesta prova?

Quan va n√©ixer el 'primer LLM', estava acabant unes pr√†ctiques en un equip que constru√Øa un model de llenguatge. No era gran, i no era un transformador, per√≤ amb retrospectiva estava relativament a prop de la revoluci√≥ dels LLM. Per√≤ no hi estava prestant gaire atenci√≥. La majoria de nosaltres no ho f√®iem.

El gener de 2018, mentre lluitava amb Golang en les meves segones pr√†ctiques, l'australi√† Jeremy Howard va publicar *ULMFit*, el primer LLM.

<figure style="margin: 0; margin-bottom: 1.4em;">
  <img
    src="/images/gpt-timeline-2.png"
    alt="Cronologia de GPT-1 a GPT-3.5"
    style="border-radius: 0.4em; border: 2px solid #ddd;"
  >
</figure>

Alec Radford va publicar *Improving Language Understanding by Generative Pre-Training* (GPT-1) l'11 de juny de 2018.

GPT-1 (Generative Pre-Train One) √©s √†mpliament acceptat com un LLM.
Segons molts, *√©s* el primer. Per√≤ Jeremy Howard afirma el contrari.
Si hem d'entendre el naixement dels LLM, haurem de definir qu√® fa exactament que un LLM sigui un LLM.

<div style="display: flex; justify-content: center;">
<blockquote class="twitter-tweet" data-lang="en" data-dnt="true"><p lang="en" dir="ltr">What I&#39;ve been working on for the past year! <a href="https://t.co/CAQMYS1rR7">https://t.co/CAQMYS1rR7</a><br><br>Inspired by CoVE, ELMo, and ULMFiT we show that a single transformer language model can be finetuned to a wide variety of NLP tasks and performs very well with little tuning/tweaking.</p>&mdash; Alec Radford (@AlecRad) <a href="https://twitter.com/AlecRad/status/1006247734691545089?ref_src=twsrc%5Etfw">June 11, 2018</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>

## Qu√® √©s un LLM?

**LLM** <span style="color: #546E7A;">(*_**substantiu**_*)</span>: un model de llenguatge que ha estat entrenat de manera tan efectiva amb auto-supervisi√≥ com a 'predictor de la seg√ºent paraula' que es pot adaptar f√†cilment i amb √®xit a moltes tasques espec√≠fiques basades en text.

Havent llegit GPT-1, 2, 3 i alguns altres articles, aquesta √©s la definici√≥ que m'agrada. No √©s una definici√≥ trivial, aix√≠ que desglossem-la:

-   **"√©s un model de llenguatge"** ‚Üí les entrades i les sortides predites s√≥n components del llenguatge hum√† escrit (per exemple, angl√®s). Aquests components no s√≥n necess√†riament, i no solen ser, paraules. Poden ser car√†cters o seq√º√®ncies de car√†cters (tokens).
-   **"entrenat amb auto-supervisi√≥"** ‚Üí el conjunt de dades √©s text *sense etiquetar* del qual es produeixen exemples (x,y). Aix√≤ va ser una sortida important dels conjunts de dades de text etiquetats, cars i espec√≠fics per a tasques.
-   **"predictor de la seg√ºent paraula"** ‚Üí al model se li d√≥na una seq√º√®ncia de paraules/car√†cters/tokens i ha de predir qu√® ve a continuaci√≥. "El gat al" ‚Üí "barret".
-   **"f√†cilment adaptable"** ‚Üí no es fan canvis arquitect√≤nics al model; el model t√© capacitats de pocs trets, fins i tot d'un sol tret.
-   **"adaptat amb √®xit"** ‚Üí aconseguir un rendiment d'√∫ltima generaci√≥
-   **"moltes tasques espec√≠fiques basades en text"** ‚Üí el model pot realitzar classificaci√≥, resposta a preguntes, an√†lisi sint√†ctic i altres desafiaments de text amb un rendiment d'√∫ltima generaci√≥. Aquest √©s un salt important m√©s enll√† dels models de llenguatge espec√≠fics per a tasques que s√≥n bons en una cosa i dolents en pr√†cticament tot el dem√©s.

He om√®s de manera consp√≠cua la part "gran" de la definici√≥ de LLM, per√≤ est√† impl√≠cita per l'√®xit de l'entrenament generatiu auto-supervisat. Abans d'una certa mida de par√†metres, aquesta arquitectura de model de llenguatge no funcionava. Avui dia, els LLM m√©s grans s√≥n 1000 vegades m√©s grans que els m√©s petits.

Tamb√© he deixat de banda qualsevol vinculaci√≥ de la categoria LLM a l'arquitectura del transformador. Tot i que √©s l'arquitectura LLM m√©s dominant, n'hi ha d'altres (LSTM, Mamba, [Diffusion](https://x.com/InceptionAILabs/status/1894847919624462794)).

Tot el dem√©s de la definici√≥ va ser, crec, clau per a GPT-1 i el 'moment LLM'.

Si llegiu els articles de GPT-2 i GPT-3, procedeixen gaireb√© directament de l'√®xit de GPT-1. Tot i que GPT-1 no inclou les paraules "model de llenguatge gran" en absolut, els articles posteriors s√≠ que ho fan i es refereixen a GPT-1 com a tal. Aix√≠ que GPT-1 √©s un LLM primerenc, i potser el primer, si els seus precedents ‚ÄîULMFit, ELMo i CoVE‚Äî no poden fer la reclamaci√≥.

## S√≥n algun de CoVE, ELMo i ULMFit LLMs?

<figure style="margin: 0; margin-bottom: 1.4em;">
  <img
    src="/images/gpt-timeline.png"
    alt="Cronologia de GPT-1 mostrant la seva relaci√≥ amb les refer√®ncies discutides en aquesta publicaci√≥."
    style="border-radius: 0.4em; border: 2px solid #ddd;"
  >
  <figcaption style="color: #777;">Cronologia de GPT-1 mostrant la seva relaci√≥ amb les refer√®ncies discutides en aquesta publicaci√≥.</figcaption>
</figure>

[Contextualized Word Vectors (CoVE)](https://arxiv.org/pdf/1708.00107) van ser una innovaci√≥ important en l'aprenentatge per transfer√®ncia, per√≤ no s'assemblen gaire a GPT-1. Els vectors CoVE es van crear amb aprenentatge supervisat (en traducci√≥ d'angl√®s a alemany) no amb aprenentatge auto-supervisat, i els vectors nom√©s esdevenen un component inicial en un model m√©s gran espec√≠fic per a tasques.

<figure style="margin: 0; margin-bottom: 1em;">
  <img
    src="/images/CoVE.png"
    alt="‚ÄúFigura 1: a) entrenem una LSTM bidireccional de dues capes com a codificador d'un model seq√º√®ncia-a-seq√º√®ncia atencional per a la traducci√≥ autom√†tica i b) la utilitzem per proporcionar context a altres models de PNL.‚Äù"
    style="border-radius: 0.4em; border: 2px solid #ddd;"
  >
  <figcaption style="color: #777;">"Figura 1: a) entrenem una LSTM bidireccional de dues capes com a codificador d'un model seq√º√®ncia-a-seq√º√®ncia atencional per a la traducci√≥ autom√†tica i b) la utilitzem per proporcionar context a altres models de PNL.‚Äù</figcaption>
</figure>

[Embeddings From Language Models (ELMo)](https://arxiv.org/pdf/1802.05365) tamb√© entrena embeddings de paraules i els acobla a models espec√≠fics per a tasques. De la secci√≥ **Treball relacionat** de GPT-1:

> [ELMo i CoVE] utilitzen representacions ocultes d'un model de llenguatge o traducci√≥ autom√†tica pre-entrenat com a caracter√≠stiques auxiliars mentre s'entrena un model supervisat en la tasca objectiu. Aix√≤ implica una quantitat substancial de nous par√†metres per a cada tasca objectiu separada, mentre que nosaltres requerim canvis m√≠nims a la nostra arquitectura de model durant la transfer√®ncia.
>

Cap d'aquests √©s un LLM segons la meva opini√≥, tot i que per a Alec Radford van ser clarament trampolins. Passem ara a ULMFit, que els autors de GPT-1 diuen que √©s la "l√≠nia de treball m√©s propera a la nostra".

Universal Language Model Fine-tuning for Text Classification (ULMFit) √©s un predictor de la seg√ºent paraula LSTM entrenat de manera auto-supervisada a WikiText, adaptable de manera econ√≤mica i sense canvis d'arquitectura per realitzar diverses tasques de classificaci√≥ de text amb un rendiment d'√∫ltima generaci√≥. (Veure com de b√© va funcionar ULMFit en el conjunt de dades de ressenyes de pel¬∑l√≠cules d'IMDb va ser un moment ü§Ø per al seu autor, Jeremy Howard.)

<figure style="margin: 0; margin-bottom: 1.4em;">
  <img
    src="/images/ulmfit-architecture.png"
    alt="Visi√≥ general de l'arquitectura ULMFit, amb les seves 'intricades esquemes d'aprenentatge' al mig i a la dreta."
    style="border-radius: 0.4em; border: 2px solid #ddd;"
  >
  <figcaption style="color: #777;">Visi√≥ general de l'arquitectura ULMFit, amb les seves "intricades esquemes d'aprenentatge‚Äù al mig i a la dreta.</figcaption>
</figure>

Aquest ULMFit s'assembla molt a GPT-1 i a la definici√≥ anterior d'un LLM. Les √∫niques parts que no s√≥n discutiblement semblants a GPT s√≥n la facilitat de l'ajust fi i l'amplitud de les tasques aplicades.
L'article de GPT-1 assenyala amb ra√≥ la complexitat de les "taxes d'aprenentatge triangulars" i el "descongelament gradual" dels par√†metres.
L'article de GPT-1 tamb√© afirma que en substituir l'arquitectura LSTM per la del transformador, desbloquegen una compet√®ncia m√©s √†mplia espec√≠fica per a tasques que ULMFit, allargant la capacitat de predicci√≥ del model pre-entrenat.

Despr√©s de remenar el passat, estic satisfet de considerar ULMFit el primer LLM. Aix√≤ √©s segurament discutible. No he prestat gaire atenci√≥ a l'article de Dai i Le del 2015 *Semi-supervised Sequence Learning* (https://arxiv.org/pdf/1511.01432), que va ser l'altra "l√≠nia de treball m√©s propera" esmentada a l'article de GPT-1. Tamb√© se li d√≥na m√©s import√†ncia que a ULMFit en la [pr√≤pia hist√≤ria de Radford del 2020](https://www.youtube.com/watch?v=BnpB3GrpsfM) del moment GPT.

Importa ser el primer? Crec que s√≠, una mica. La ind√∫stria del programari i l'acad√®mia honoren els seus fundadors. Tots formem part d'una cultura que [colonitza la noosfera](https://en.wikipedia.org/wiki/Homesteading_the_Noosphere).


<div style="display: flex; justify-content: center;">
<blockquote class="twitter-tweet" data-conversation="none" data-dnt="true"><p lang="en" dir="ltr">ofc the real reason I&#39;m pushing back on this is that I&#39;m worried some folks might realise I didn&#39;t actually create the first LLM at on my own at <a href="https://t.co/GEOZunWoXj">https://t.co/GEOZunWoXj</a>, but that actually I&#39;m just a token figurehead for a CCP conspiracy that created it &amp; had me take credit</p>&mdash; Jeremy Howard (@jeremyphoward) <a href="https://twitter.com/jeremyphoward/status/1882964239520071926?ref_src=twsrc%5Etfw">January 25, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>

A m√©s, entendre el llinatge no-OpenAI dels LLM ajuda a comprendre la din√†mica competitiva de la ind√∫stria. Tot i que OpenAI va agafar els seus competidors desprevinguts, la hist√≤ria dels LLM sempre ha estat multipolar, multigeneracional i geogr√†ficament diversa. Amb retrospectiva, podem preguntar-nos: si els australians poden fer avan√ßar l'estat de l'art, per qu√® no la Xina?


## L'√∫ltim LLM

<script type="text/javascript" src="https://ssl.gstatic.com/trends_nrtr/4017_RC01/embed_loader.js"></script> <script type="text/javascript"> trends.embed.renderExploreWidget("TIMESERIES", {"comparisonItem":[{"keyword":"LLM","geo":"US","time":"today 5-y"}],"category":0,"property":""}, {"exploreQuery":"date=today%205-y&geo=US&q=LLM&hl=en","guestPath":"https://trends.google.com:443/trends/embed/"}); </script>

Havent tornat a l'inici de la bogeria dels LLM, m'ha fet preguntar-me quan veurem el final. GPT-4V va ser la introducci√≥ de les capacitats de comprensi√≥ d'imatges a la fam√≠lia de models anteriorment nom√©s de text, i des de llavors els 'laboratoris de frontera' s'han tornat multimodals. Amb ChatGPT, Claude i Gemini afegint processament d'imatges i √†udio, ja no sembla adequat anomenar-los models de llenguatge. En lloc de LLM, estem veient un √∫s creixent (per√≤ encara menor) de "model fundacional".

Si hagu√©s d'endevinar, diria que el terme LLM es mantindr√†. Es convertir√† en com la unitat de processament gr√†fic (GPU). El p√∫blic en general acabar√† utilitzant aquests models com a v√≠deo-entrada v√≠deo-sortida, i els anomenaran LLM. El que va comen√ßar com un terme per a alguna cosa que analitzava ressenyes de pel¬∑l√≠cules d'IMDb es convertir√† en un terme per a alguna cosa que fa pel¬∑l√≠cules. Almenys, aix√≠ √©s com ho pensar√© jo.

El primer LLM va ser una LSTM pre-entrenada a Wikipedia i ajustada a ressenyes de pel¬∑l√≠cules d'IMDb. GPT-1 va substituir crucialment l'arquitectura LSTM per la del transformador, eliminant la complexitat d'ULMFit i oferint a la ind√∫stria una rampa d'escalada que s'estendr√† [almenys fins al 2030](https://epoch.ai/blog/can-ai-scaling-continue-through-2030). Molts, molts m√©s LLM per venir.

Prepareu-vos i mantingueu l'atenci√≥ a la carretera que teniu al davant.


<figure style="margin: 0; margin-top: 1.4em; margin-bottom: 1.4em;" class="magnify-container">
  <img
    src="/images/scaling-till-2030.png"
    alt="Cronologia estesa de LLM fins al 2020, mostrant quant de cam√≠ queda per davant."
    style="border-radius: 0.4em; border: 2px solid #ddd;"
    class="magnify-image"
  >
  <div class="magnifying-glass"></div>
  <style>
    @media (hover: hover) {
      .magnify-container {
        position: relative;
        overflow: visible;
      }

      .magnifying-glass {
        display: none;
        position: absolute;
        width: 150px;
        height: 150px;
        border: 2px solid #fff;
        border-radius: 50%;
        pointer-events: none;
        background-repeat: no-repeat;
        box-shadow: 0 0 0 7px rgba(255, 255, 255, 0.85),
                    0 0 7px 7px rgba(0, 0, 0, 0.25),
                    inset 0 0 40px 2px rgba(0, 0, 0, 0.25);
      }

      .magnify-container:hover .magnifying-glass {
        display: block;
      }
    }
  </style>
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      const containers = document.querySelectorAll('.magnify-container');

      containers.forEach(container => {
        const img = container.querySelector('.magnify-image');
        const glass = container.querySelector('.magnifying-glass');

        container.addEventListener('mousemove', function(e) {
          const bounds = container.getBoundingClientRect();
          const x = e.clientX - bounds.left;
          const y = e.clientY - bounds.top;

          const magnification = 2;
          const glassRadius = glass.offsetWidth / 2;

          glass.style.left = `${x - glassRadius}px`;
          glass.style.top = `${y - glassRadius}px`;

          const bgX = x * magnification - glassRadius;
          const bgY = y * magnification - glassRadius;

          glass.style.backgroundImage = `url(${img.src})`;
          glass.style.backgroundSize = `${img.width * magnification}px ${img.height * magnification}px`;
          glass.style.backgroundPosition = `-${bgX}px -${bgY}px`;
        });
      });
    });
  </script>
</figure>


----

**Actualitzaci√≥:** @levelsio i Jeremy Howard [van discutir sobre aquesta q√ºesti√≥](https://x.com/jonobelotti_IO/status/1906086472349786595) el 29 de mar√ß de 2025. D'aix√≤ va sorgir el feedback que hauria de prestar m√©s atenci√≥ a BERT ("La primera demostraci√≥ d'ULMFiT va precedir BERT.") i a l'article del 2015 *Semi-supervised Sequence Learning*.
